{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y7i2u4g-qts"
      },
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Models\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "#Result Representation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9srxKLU-1aN",
        "outputId": "7d5f1ecf-a8d9-4eff-85dd-8353036c95af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/chat_dataset.csv\")"
      ],
      "metadata": {
        "id": "aKan38eE-5ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding"
      ],
      "metadata": {
        "id": "C5AsH2Sa_Kuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tneg= df[df['sentiment'] =='negative']\n",
        "tpos= df[df['sentiment'] == \"positive\"]\n",
        "tneu= df[df['sentiment'] == \"neutral\"]\n"
      ],
      "metadata": {
        "id": "PIqhV_SL_0fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlist = tneg['message'].tolist()\n",
        "plist = tpos['message'].tolist()\n",
        "neulist = tneu['message'].tolist()"
      ],
      "metadata": {
        "id": "yD3fTYRs_2ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nstr= ' '.join(nlist)\n",
        "pstr= ' '.join(plist)\n",
        "neustr= ' '.join(neulist)"
      ],
      "metadata": {
        "id": "UF2dbUHz_7AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nwords = nstr.lower().split(\" \")\n",
        "pwords = pstr.lower().split(\" \")\n",
        "neuwords = neustr.lower().split(\" \")"
      ],
      "metadata": {
        "id": "tIYV7JC3_qf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(text):\n",
        "    global word_encoding\n",
        "    bag = {}\n",
        "    words = text.split()  # split text into words\n",
        "    for word in words:\n",
        "        if word in vocab:\n",
        "            encoding = vocab[word]  # get encoding from vocab\n",
        "        else:\n",
        "            vocab[word] = word_encoding\n",
        "            encoding = word_encoding\n",
        "            word_encoding += 1\n",
        "\n",
        "        if encoding in bag:\n",
        "            bag[encoding] += 1\n",
        "        else:\n",
        "            bag[encoding] = 1\n",
        "\n",
        "    return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq3ONz7rAP0P",
        "outputId": "49f7b750-d295-43a0-d82b-51c67c413e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{8: 2, 11: 3, 12: 3, 380: 3, 20: 1, 381: 1, 54: 1, 382: 1, 178: 1}\n",
            "{\"i'm\": 1, 'not': 2, 'sure': 3, 'how': 4, 'i': 5, 'feel': 6, 'about': 7, 'this': 8, 'the': 9, 'price': 10, 'is': 11, 'a': 12, 'bit': 13, 'high': 14, 'feeling': 15, 'neutral': 16, \"don't\": 17, 'know': 18, 'what': 19, 'to': 20, 'think': 21, 'have': 22, 'an': 23, 'opinion': 24, 'on': 25, 'really': 26, 'care': 27, 'undecided': 28, 'indifferent': 29, 'strong': 30, 'feelings': 31, 'mixed': 32, 'one': 33, 'way': 34, 'or': 35, 'other': 36, 'make': 37, 'of': 38, 'noise': 39, 'level': 40, 'tolerable': 41, 'no': 42, 'food': 43, 'was': 44, 'average': 45, 'view': 46, 'from': 47, 'here': 48, 'topic': 49, 'scenery': 50, 'preference': 51, 'performance': 52, 'mediocre': 53, 'if': 54, 'like': 55, 'traffic': 56, 'moderate': 57, 'interest': 58, 'in': 59, 'either': 60, 'ambivalent': 61, 'service': 62, 'at': 63, 'restaurant': 64, 'movie': 65, 'okay': 66, 'book': 67, 'want': 68, 'pursue': 69, 'bad': 70, 'hotel': 71, 'phone': 72, 'just': 73, 'interested': 74, 'unremarkable': 75, 'noisy': 76, 'during': 77, 'day': 78, 'unsure': 79, 'unimpressive': 80, 'fence': 81, 'subject': 82, 'hike': 83, 'uneventful': 84, 'product': 85, 'good': 86, 'satisfactory': 87, 'typical': 88, 'for': 89, 'time': 90, 'beach': 91, 'museum': 92, 'software': 93, 'game': 94, 'speaker': 95, 'emotions': 96, 'car': 97, 'company': 98, 'weather': 99, 'today': 100, 'particularly': 101, 'standard': 102, 'normal': 103, 'strongly': 104, 'particular': 105, 'regular': 106, 'opinions': 107, 'ordinary': 108, 'hey..': 109, \"what's\": 110, 'up?': 111, 'ğŸ˜€': 112, 'so-so': 113, 'ğŸ˜•': 114, 'thanks': 115, 'your': 116, 'help!': 117, 'ğŸ™': 118, 'do': 119, 'can': 120, 'we': 121, 'reschedule': 122, 'our': 123, 'meeting?': 124, 'ğŸ“…': 125, 'my': 126, 'way!': 127, 'ğŸš—': 128, 'crazy': 129, 'â˜€ï¸ğŸŒ§ï¸': 130, 'great': 131, 'ğŸ˜·': 132, 'looking': 133, 'forward': 134, 'vacation!': 135, 'ğŸŒ´ğŸ–ï¸': 136, 'eat': 137, 'ğŸ½ï¸': 138, 'middle': 139, 'something.': 140, 'call': 141, 'you': 142, 'back': 143, 'later?': 144, 'ğŸ“±': 145, 'trying': 146, 'decide': 147, 'between': 148, 'two': 149, 'options': 150, 'ğŸ¤”': 151, 'it': 152, 'nice': 153, 'meeting': 154, 'you!': 155, 'ğŸ‘‹': 156, 'hanging': 157, 'out': 158, 'home': 159, 'ğŸ ': 160, 'mean': 161, 'ğŸ¤·\\u200dâ™€ï¸': 162, 'sorry.': 163, \"can't\": 164, 'party': 165, 'ğŸ‰': 166, 'so': 167, 'tired': 168, 'ğŸ˜´': 169, 'remind': 170, 'me': 171, 'details?': 172, 'very': 173, 'productive': 174, 'ğŸ“‰': 175, 'doing': 176, 'some': 177, 'work': 178, 'ğŸ’»': 179, 'that': 180, 'ğŸ•‘': 181, 'right': 182, 'ğŸ¤·\\u200dâ™‚ï¸': 183, 'overwhelmed': 184, 'ğŸ˜°': 185, 'wear': 186, 'ğŸ‘—': 187, 'thinking': 188, 'going': 189, 'run': 190, 'ğŸƒ\\u200dâ™€ï¸': 191, 'ready': 192, 'with': 193, 'friends': 194, 'tonight': 195, 'ğŸ»': 196, 'hungry': 197, 'ğŸ”': 198, 'better': 199, 'now': 200, 'ğŸ˜Š': 201, 'well': 202, 'ğŸ¤’': 203, 'stay': 204, 'positive': 205, 'say': 206, 'pretty': 207, 'ğŸ‘': 208, 'lost': 209, 'ğŸŒªï¸': 210, 'taking': 211, 'easy': 212, 'ğŸ›€': 213, 'ğŸ™ˆ': 214, 'anxious': 215, 'ğŸ˜¬': 216, 'next': 217, 'respond': 218, 'enjoying': 219, 'â˜€ï¸': 220, 'under': 221, 'ğŸ¤§': 222, 'should': 223, 'go': 224, 'gym': 225, 'ğŸ˜“': 226, 'relaxing': 227, 'ğŸ›‹ï¸': 228, 'bored': 229, 'ğŸ˜': 230, 'up': 231, 'focused': 232, 'ğŸ§': 233, 'stressed': 234, 'ğŸ˜«': 235, 'decision': 236, 'distracted': 237, 'ğŸ¤¯': 238, 'ğŸ¤¨': 239, 'family': 240, 'ğŸ‘¨\\u200dğŸ‘©\\u200dğŸ‘§\\u200dğŸ‘¦': 241, 'uncertain': 242, 'things': 243, 'dinner': 244, 'ğŸ“†': 245, 'restless': 246, 'ğŸ¤«': 247, 'mood': 248, 'ğŸ˜Œ': 249, 'comfort': 250, 'zone': 251, 'event': 252, 'alone': 253, 'ğŸ§˜\\u200dâ™€ï¸': 254, 'stuck': 255, 'ğŸš¶\\u200dâ™€ï¸': 256, 'free': 257, 'figure': 258, 'ğŸ”': 259, 'everything': 260, 'ğŸ˜©': 261, 'making': 262, 'step': 263, 'ğŸš¶\\u200dâ™‚ï¸': 264, 'frustrated': 265, 'ğŸ˜¤': 266, 'anything': 267, 'keep': 268, 'perspective': 269, 'ğŸŒ…': 270, 'confused': 271, 'goals': 272, 'ğŸ¯': 273, 'without': 274, 'find': 275, 'ğŸ—ºï¸': 276, 'routine': 277, 'ğŸ¥±': 278, 'spare': 279, 'head': 280, 'above': 281, 'water': 282, 'ğŸŒŠ': 283, 'ğŸ•º': 284, 'challenge': 285, 'and': 286, 'moving': 287, 'future': 288, 'life': 289, 'most': 290, 'every': 291, 'ğŸŒ': 292, 'plate': 293, 'take': 294, 'risk': 295, 'ğŸ’¼': 296, 'weekend': 297, 'balance': 298, 'âš–ï¸': 299, 'plan': 300, 'ğŸ“': 301, 'reach': 302, 'them': 303, 'need': 304, 'buy': 305, '5': 306, 'oranges': 307, 'recipe': 308, 'ğŸŠ': 309, 'send': 310, 'link': 311, 'article': 312, 'mentioned': 313, 'earlier?': 314, 'ğŸ“ğŸ¤”': 315, 'dentist': 316, 'appointment': 317, 'tomorrow': 318, '2pm': 319, 'ğŸ¦·ğŸ˜¬': 320, 'plants': 321, 'before': 322, 'leave': 323, 'ğŸŒ±ğŸ’¦': 324, 'grocery': 325, 'store': 326, 'after': 327, 'milk': 328, 'bread': 329, 'ğŸ›ï¸ğŸ': 330, 'working': 331, 'project': 332, 'deadline': 333, 'week': 334, 'ğŸ—“ï¸ğŸ“ˆ': 335, 'break': 336, 'stretch': 337, 'legs': 338, 'ğŸš¶\\u200dâ™€ï¸ğŸ’ª': 339, 'clean': 340, 'apartment': 341, 'parents': 342, 'come': 343, 'visit': 344, 'ğŸ§¹ğŸ ': 345, 'watching': 346, 'eating': 347, 'popcorn': 348, 'ğŸ¿ğŸ¥': 349, 'insurance': 350, 'update': 351, 'policy': 352, 'ğŸ“ğŸ¤”': 353, 'studying': 354, 'final': 355, 'exams': 356, 'ğŸ“šğŸ“': 357, 'planning': 358, 'road': 359, 'trip': 360, 'summer': 361, 'ğŸš—ğŸŒ´': 362, 'browsing': 363, 'internet': 364, 'ideas': 365, 'decorate': 366, 'ğŸ–¥ï¸ğŸ›‹ï¸': 367, 'laundry': 368, 'folding': 369, 'clothes': 370, 'ğŸ§ºğŸ‘•': 371, 'new': 372, 'shoes': 373, 'work.': 374, 'old': 375, 'ones': 376, 'are': 377, 'worn': 378, 'ğŸ‘ğŸ›ï¸': 379, 'test': 380, 'see': 381, 'will': 382}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "  encoded_text = encode(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred)\n",
        "  print(result[0])\n"
      ],
      "metadata": {
        "id": "8XM2XzRXERhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "NzNnQ4qhFm_i",
        "outputId": "d8968810-4ff7-4b56-b9e3-03a590046b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'dict' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-afbbb6f430ea>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpositive_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"That movie was! really loved it and would great watch it again because it was amazingly great\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-cb99482506ec>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mencoded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
          ]
        }
      ]
    }
  ]
}