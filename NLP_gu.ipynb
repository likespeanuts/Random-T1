{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y7i2u4g-qts"
      },
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Models\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "#Result Representation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9srxKLU-1aN",
        "outputId": "7d5f1ecf-a8d9-4eff-85dd-8353036c95af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/chat_dataset.csv\")"
      ],
      "metadata": {
        "id": "aKan38eE-5ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding"
      ],
      "metadata": {
        "id": "C5AsH2Sa_Kuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tneg= df[df['sentiment'] =='negative']\n",
        "tpos= df[df['sentiment'] == \"positive\"]\n",
        "tneu= df[df['sentiment'] == \"neutral\"]\n"
      ],
      "metadata": {
        "id": "PIqhV_SL_0fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlist = tneg['message'].tolist()\n",
        "plist = tpos['message'].tolist()\n",
        "neulist = tneu['message'].tolist()"
      ],
      "metadata": {
        "id": "yD3fTYRs_2ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nstr= ' '.join(nlist)\n",
        "pstr= ' '.join(plist)\n",
        "neustr= ' '.join(neulist)"
      ],
      "metadata": {
        "id": "UF2dbUHz_7AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nwords = nstr.lower().split(\" \")\n",
        "pwords = pstr.lower().split(\" \")\n",
        "neuwords = neustr.lower().split(\" \")"
      ],
      "metadata": {
        "id": "tIYV7JC3_qf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(text):\n",
        "    global word_encoding\n",
        "    bag = {}\n",
        "    words = text.split()  # split text into words\n",
        "    for word in words:\n",
        "        if word in vocab:\n",
        "            encoding = vocab[word]  # get encoding from vocab\n",
        "        else:\n",
        "            vocab[word] = word_encoding\n",
        "            encoding = word_encoding\n",
        "            word_encoding += 1\n",
        "\n",
        "        if encoding in bag:\n",
        "            bag[encoding] += 1\n",
        "        else:\n",
        "            bag[encoding] = 1\n",
        "\n",
        "    return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq3ONz7rAP0P",
        "outputId": "49f7b750-d295-43a0-d82b-51c67c413e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{8: 2, 11: 3, 12: 3, 380: 3, 20: 1, 381: 1, 54: 1, 382: 1, 178: 1}\n",
            "{\"i'm\": 1, 'not': 2, 'sure': 3, 'how': 4, 'i': 5, 'feel': 6, 'about': 7, 'this': 8, 'the': 9, 'price': 10, 'is': 11, 'a': 12, 'bit': 13, 'high': 14, 'feeling': 15, 'neutral': 16, \"don't\": 17, 'know': 18, 'what': 19, 'to': 20, 'think': 21, 'have': 22, 'an': 23, 'opinion': 24, 'on': 25, 'really': 26, 'care': 27, 'undecided': 28, 'indifferent': 29, 'strong': 30, 'feelings': 31, 'mixed': 32, 'one': 33, 'way': 34, 'or': 35, 'other': 36, 'make': 37, 'of': 38, 'noise': 39, 'level': 40, 'tolerable': 41, 'no': 42, 'food': 43, 'was': 44, 'average': 45, 'view': 46, 'from': 47, 'here': 48, 'topic': 49, 'scenery': 50, 'preference': 51, 'performance': 52, 'mediocre': 53, 'if': 54, 'like': 55, 'traffic': 56, 'moderate': 57, 'interest': 58, 'in': 59, 'either': 60, 'ambivalent': 61, 'service': 62, 'at': 63, 'restaurant': 64, 'movie': 65, 'okay': 66, 'book': 67, 'want': 68, 'pursue': 69, 'bad': 70, 'hotel': 71, 'phone': 72, 'just': 73, 'interested': 74, 'unremarkable': 75, 'noisy': 76, 'during': 77, 'day': 78, 'unsure': 79, 'unimpressive': 80, 'fence': 81, 'subject': 82, 'hike': 83, 'uneventful': 84, 'product': 85, 'good': 86, 'satisfactory': 87, 'typical': 88, 'for': 89, 'time': 90, 'beach': 91, 'museum': 92, 'software': 93, 'game': 94, 'speaker': 95, 'emotions': 96, 'car': 97, 'company': 98, 'weather': 99, 'today': 100, 'particularly': 101, 'standard': 102, 'normal': 103, 'strongly': 104, 'particular': 105, 'regular': 106, 'opinions': 107, 'ordinary': 108, 'hey..': 109, \"what's\": 110, 'up?': 111, '😀': 112, 'so-so': 113, '😕': 114, 'thanks': 115, 'your': 116, 'help!': 117, '🙏': 118, 'do': 119, 'can': 120, 'we': 121, 'reschedule': 122, 'our': 123, 'meeting?': 124, '📅': 125, 'my': 126, 'way!': 127, '🚗': 128, 'crazy': 129, '☀️🌧️': 130, 'great': 131, '😷': 132, 'looking': 133, 'forward': 134, 'vacation!': 135, '🌴🏖️': 136, 'eat': 137, '🍽️': 138, 'middle': 139, 'something.': 140, 'call': 141, 'you': 142, 'back': 143, 'later?': 144, '📱': 145, 'trying': 146, 'decide': 147, 'between': 148, 'two': 149, 'options': 150, '🤔': 151, 'it': 152, 'nice': 153, 'meeting': 154, 'you!': 155, '👋': 156, 'hanging': 157, 'out': 158, 'home': 159, '🏠': 160, 'mean': 161, '🤷\\u200d♀️': 162, 'sorry.': 163, \"can't\": 164, 'party': 165, '🎉': 166, 'so': 167, 'tired': 168, '😴': 169, 'remind': 170, 'me': 171, 'details?': 172, 'very': 173, 'productive': 174, '📉': 175, 'doing': 176, 'some': 177, 'work': 178, '💻': 179, 'that': 180, '🕑': 181, 'right': 182, '🤷\\u200d♂️': 183, 'overwhelmed': 184, '😰': 185, 'wear': 186, '👗': 187, 'thinking': 188, 'going': 189, 'run': 190, '🏃\\u200d♀️': 191, 'ready': 192, 'with': 193, 'friends': 194, 'tonight': 195, '🍻': 196, 'hungry': 197, '🍔': 198, 'better': 199, 'now': 200, '😊': 201, 'well': 202, '🤒': 203, 'stay': 204, 'positive': 205, 'say': 206, 'pretty': 207, '👍': 208, 'lost': 209, '🌪️': 210, 'taking': 211, 'easy': 212, '🛀': 213, '🙈': 214, 'anxious': 215, '😬': 216, 'next': 217, 'respond': 218, 'enjoying': 219, '☀️': 220, 'under': 221, '🤧': 222, 'should': 223, 'go': 224, 'gym': 225, '😓': 226, 'relaxing': 227, '🛋️': 228, 'bored': 229, '😐': 230, 'up': 231, 'focused': 232, '🧐': 233, 'stressed': 234, '😫': 235, 'decision': 236, 'distracted': 237, '🤯': 238, '🤨': 239, 'family': 240, '👨\\u200d👩\\u200d👧\\u200d👦': 241, 'uncertain': 242, 'things': 243, 'dinner': 244, '📆': 245, 'restless': 246, '🤫': 247, 'mood': 248, '😌': 249, 'comfort': 250, 'zone': 251, 'event': 252, 'alone': 253, '🧘\\u200d♀️': 254, 'stuck': 255, '🚶\\u200d♀️': 256, 'free': 257, 'figure': 258, '🔍': 259, 'everything': 260, '😩': 261, 'making': 262, 'step': 263, '🚶\\u200d♂️': 264, 'frustrated': 265, '😤': 266, 'anything': 267, 'keep': 268, 'perspective': 269, '🌅': 270, 'confused': 271, 'goals': 272, '🎯': 273, 'without': 274, 'find': 275, '🗺️': 276, 'routine': 277, '🥱': 278, 'spare': 279, 'head': 280, 'above': 281, 'water': 282, '🌊': 283, '🕺': 284, 'challenge': 285, 'and': 286, 'moving': 287, 'future': 288, 'life': 289, 'most': 290, 'every': 291, '🌞': 292, 'plate': 293, 'take': 294, 'risk': 295, '💼': 296, 'weekend': 297, 'balance': 298, '⚖️': 299, 'plan': 300, '📝': 301, 'reach': 302, 'them': 303, 'need': 304, 'buy': 305, '5': 306, 'oranges': 307, 'recipe': 308, '🍊': 309, 'send': 310, 'link': 311, 'article': 312, 'mentioned': 313, 'earlier?': 314, '📝🤔': 315, 'dentist': 316, 'appointment': 317, 'tomorrow': 318, '2pm': 319, '🦷😬': 320, 'plants': 321, 'before': 322, 'leave': 323, '🌱💦': 324, 'grocery': 325, 'store': 326, 'after': 327, 'milk': 328, 'bread': 329, '🛍️🍞': 330, 'working': 331, 'project': 332, 'deadline': 333, 'week': 334, '🗓️📈': 335, 'break': 336, 'stretch': 337, 'legs': 338, '🚶\\u200d♀️💪': 339, 'clean': 340, 'apartment': 341, 'parents': 342, 'come': 343, 'visit': 344, '🧹🏠': 345, 'watching': 346, 'eating': 347, 'popcorn': 348, '🍿🎥': 349, 'insurance': 350, 'update': 351, 'policy': 352, '📞🤔': 353, 'studying': 354, 'final': 355, 'exams': 356, '📚📝': 357, 'planning': 358, 'road': 359, 'trip': 360, 'summer': 361, '🚗🌴': 362, 'browsing': 363, 'internet': 364, 'ideas': 365, 'decorate': 366, '🖥️🛋️': 367, 'laundry': 368, 'folding': 369, 'clothes': 370, '🧺👕': 371, 'new': 372, 'shoes': 373, 'work.': 374, 'old': 375, 'ones': 376, 'are': 377, 'worn': 378, '👞🛍️': 379, 'test': 380, 'see': 381, 'will': 382}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "  encoded_text = encode(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred)\n",
        "  print(result[0])\n"
      ],
      "metadata": {
        "id": "8XM2XzRXERhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "NzNnQ4qhFm_i",
        "outputId": "d8968810-4ff7-4b56-b9e3-03a590046b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'dict' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-afbbb6f430ea>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpositive_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"That movie was! really loved it and would great watch it again because it was amazingly great\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-cb99482506ec>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mencoded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
          ]
        }
      ]
    }
  ]
}